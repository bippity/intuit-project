{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training arrays using pickle and now test a couple diffrent moddels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it time to train our model\n",
    "\n",
    "We start by normalizing the data by scaling it, min is 0 and max is 255 for pixel data So we will divide it by 255, Keras also has a built in function to do this\n",
    "\n",
    "Our model will be Sequential\n",
    "\n",
    "First layer is 64 with a window size of 3,3 The activation function is rectified linear (relu) and finaly a pooling size of 2 by 2\n",
    "\n",
    "2nd layer will be the same as layer one\n",
    "\n",
    "Then we will flatten the data since the Convolutional is in 2D and that dense layer need a 1D\n",
    "\n",
    "Layer 3 will be the Dense layer of size 64\n",
    "\n",
    "Layer 4 will be our last layer with size 1 (Good or bad) and its activation layer will be the Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255 #255 pixels max for pixel data \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,3), input_shape = X.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# have to flatten b/c Convolutional is 2D and dense layer want a 1D\n",
    "model.add(Flatten()) \n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# output layer \n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1:\n",
    "\n",
    "Optimizer = SGD, with custom paramaters \n",
    "\n",
    "Loss Function = Binary Crossentropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3361 samples, validate on 374 samples\n",
      "Epoch 1/5\n",
      "3361/3361 [==============================] - 138s 41ms/sample - loss: nan - accuracy: 0.5040 - val_loss: nan - val_accuracy: 0.5455\n",
      "Epoch 2/5\n",
      "3361/3361 [==============================] - 138s 41ms/sample - loss: nan - accuracy: 0.5040 - val_loss: nan - val_accuracy: 0.5455\n",
      "Epoch 3/5\n",
      "3361/3361 [==============================] - 140s 42ms/sample - loss: nan - accuracy: 0.5040 - val_loss: nan - val_accuracy: 0.5455\n",
      "Epoch 4/5\n",
      "3361/3361 [==============================] - 138s 41ms/sample - loss: nan - accuracy: 0.5040 - val_loss: nan - val_accuracy: 0.5455\n",
      "Epoch 5/5\n",
      "3361/3361 [==============================] - 147s 44ms/sample - loss: nan - accuracy: 0.5040 - val_loss: nan - val_accuracy: 0.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6d3ef4490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-2, momentum=0.9)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=sgd , metrics = ['accuracy'])\n",
    "model.fit(X, y, batch_size=5, epochs = 5 , validation_split = 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: BinCross_SGD_Model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('BinCross_SGD_Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2:\n",
    "\n",
    "Optimizer = SGD\n",
    "\n",
    "Loss function = meas squared error \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3361 samples, validate on 374 samples\n",
      "Epoch 1/5\n",
      "3361/3361 [==============================] - 124s 37ms/sample - loss: 0.2500 - accuracy: 0.5132 - val_loss: 0.2496 - val_accuracy: 0.5455\n",
      "Epoch 2/5\n",
      "3361/3361 [==============================] - 130s 39ms/sample - loss: 0.2499 - accuracy: 0.5251 - val_loss: 0.2496 - val_accuracy: 0.5455\n",
      "Epoch 3/5\n",
      "3361/3361 [==============================] - 118s 35ms/sample - loss: 0.2499 - accuracy: 0.5025 - val_loss: 0.2495 - val_accuracy: 0.5455\n",
      "Epoch 4/5\n",
      "3361/3361 [==============================] - 121s 36ms/sample - loss: 0.2499 - accuracy: 0.5126 - val_loss: 0.2497 - val_accuracy: 0.5455\n",
      "Epoch 5/5\n",
      "3361/3361 [==============================] - 125s 37ms/sample - loss: 0.2498 - accuracy: 0.5147 - val_loss: 0.2497 - val_accuracy: 0.7193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6d6825190>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "#sgd = SGD(lr=0.1, decay=1e-2, momentum=0.9)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer= 'sgd' , metrics = ['accuracy'])\n",
    "model.fit(X, y, batch_size = 12, epochs = 5 , validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MeanSquaredError_SGD_Model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('MeanSquaredError_SGD_Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3:\n",
    "\n",
    "Optimizer = adam \n",
    "\n",
    "Loss Function = mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3361 samples, validate on 374 samples\n",
      "Epoch 1/5\n",
      "3361/3361 [==============================] - 126s 38ms/sample - loss: 0.2500 - accuracy: 0.5040 - val_loss: 0.2499 - val_accuracy: 0.5455\n",
      "Epoch 2/5\n",
      "3361/3361 [==============================] - 124s 37ms/sample - loss: 0.2500 - accuracy: 0.4987 - val_loss: 0.2497 - val_accuracy: 0.5455\n",
      "Epoch 3/5\n",
      "3361/3361 [==============================] - 125s 37ms/sample - loss: 0.2500 - accuracy: 0.5010 - val_loss: 0.2497 - val_accuracy: 0.5455\n",
      "Epoch 4/5\n",
      "3361/3361 [==============================] - 123s 37ms/sample - loss: 0.2500 - accuracy: 0.5040 - val_loss: 0.2497 - val_accuracy: 0.5455\n",
      "Epoch 5/5\n",
      "3361/3361 [==============================] - 123s 36ms/sample - loss: 0.2500 - accuracy: 0.5040 - val_loss: 0.2497 - val_accuracy: 0.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x69f6e6510>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer= 'adam' , metrics = ['accuracy'])\n",
    "model.fit(X, y, batch_size = 12, epochs = 5 , validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MeanSquaredError_Adam/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"MeanSquaredError_Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4:\n",
    "\n",
    "Optimizer = adam\n",
    "\n",
    "Loss function = meas absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3361 samples, validate on 374 samples\n",
      "Epoch 1/5\n",
      "3361/3361 [==============================] - 135s 40ms/sample - loss: 0.5000 - accuracy: 0.4999 - val_loss: 0.4995 - val_accuracy: 0.5455\n",
      "Epoch 2/5\n",
      "3361/3361 [==============================] - 141s 42ms/sample - loss: 0.5000 - accuracy: 0.5040 - val_loss: 0.4994 - val_accuracy: 0.5455\n",
      "Epoch 3/5\n",
      "3361/3361 [==============================] - 147s 44ms/sample - loss: 0.5000 - accuracy: 0.5040 - val_loss: 0.4992 - val_accuracy: 0.5455\n",
      "Epoch 4/5\n",
      "3361/3361 [==============================] - 124s 37ms/sample - loss: 0.5000 - accuracy: 0.5040 - val_loss: 0.4991 - val_accuracy: 0.5455\n",
      "Epoch 5/5\n",
      "3361/3361 [==============================] - 124s 37ms/sample - loss: 0.4999 - accuracy: 0.5040 - val_loss: 0.4988 - val_accuracy: 0.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x65a595690>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mean_absolute_error\", optimizer= 'adam' , metrics = ['accuracy'])\n",
    "model.fit(X, y, batch_size = 12, epochs = 5 , validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MeanAbsoluteError_Adam/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"MeanAbsoluteError_Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 5:\n",
    "\n",
    "Optimizer = adam\n",
    "\n",
    "Loss function = poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3361 samples, validate on 374 samples\n",
      "Epoch 1/5\n",
      "3361/3361 [==============================] - 127s 38ms/sample - loss: 0.8439 - accuracy: 0.4945 - val_loss: 0.8149 - val_accuracy: 0.5455\n",
      "Epoch 2/5\n",
      "3361/3361 [==============================] - 138s 41ms/sample - loss: 0.8438 - accuracy: 0.5040 - val_loss: 0.8147 - val_accuracy: 0.5455\n",
      "Epoch 3/5\n",
      "3361/3361 [==============================] - 126s 37ms/sample - loss: 0.8438 - accuracy: 0.4999 - val_loss: 0.8145 - val_accuracy: 0.5455\n",
      "Epoch 4/5\n",
      "3361/3361 [==============================] - 124s 37ms/sample - loss: 0.8438 - accuracy: 0.5040 - val_loss: 0.8147 - val_accuracy: 0.5455\n",
      "Epoch 5/5\n",
      "3361/3361 [==============================] - 136s 40ms/sample - loss: 0.8438 - accuracy: 0.5016 - val_loss: 0.8146 - val_accuracy: 0.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x764be5150>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"poisson\", optimizer= 'adam' , metrics = ['accuracy'])\n",
    "model.fit(X, y, batch_size = 12, epochs = 5 , validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Poisson_adam/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Poisson_adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 6:\n",
    "\n",
    "Optimizer = SGD with a learning rate of .001\n",
    "\n",
    "Loss function = Binary Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3361 samples, validate on 374 samples\n",
      "Epoch 1/5\n",
      "3361/3361 [==============================] - 139s 41ms/sample - loss: 0.6933 - accuracy: 0.5040 - val_loss: 0.6926 - val_accuracy: 0.5455\n",
      "Epoch 2/5\n",
      "3361/3361 [==============================] - 140s 42ms/sample - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6926 - val_accuracy: 0.5455\n",
      "Epoch 3/5\n",
      "3361/3361 [==============================] - 137s 41ms/sample - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6925 - val_accuracy: 0.5455\n",
      "Epoch 4/5\n",
      "3361/3361 [==============================] - 137s 41ms/sample - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6925 - val_accuracy: 0.5455\n",
      "Epoch 5/5\n",
      "3361/3361 [==============================] - 136s 40ms/sample - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6925 - val_accuracy: 0.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x65a32fb90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-2, momentum=0.9)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=sgd , metrics = ['accuracy'])\n",
    "model.fit(X, y, batch_size=5, epochs = 5 , validation_split = 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Binary_SmallLearningRate_SGD/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Binary_SmallLearningRate_SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 7:\n",
    "\n",
    "Optimizer = SGD with a learning rate of .0001\n",
    "\n",
    "Loss function = Binary Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3361 samples, validate on 374 samples\n",
      "Epoch 1/5\n",
      "3361/3361 [==============================] - 140s 42ms/sample - loss: 0.6932 - accuracy: 0.4891 - val_loss: 0.6931 - val_accuracy: 0.5455\n",
      "Epoch 2/5\n",
      "3361/3361 [==============================] - 138s 41ms/sample - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5455\n",
      "Epoch 3/5\n",
      "3361/3361 [==============================] - 138s 41ms/sample - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5455\n",
      "Epoch 4/5\n",
      "3361/3361 [==============================] - 137s 41ms/sample - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5455\n",
      "Epoch 5/5\n",
      "3361/3361 [==============================] - 137s 41ms/sample - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x65b559e10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGD(lr=0.0001, decay=1e-2, momentum=0.9)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=sgd , metrics = ['accuracy'])\n",
    "model.fit(X, y, batch_size=5, epochs = 5 , validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR2 = \"/Users/macbook/OCR/testData\"\n",
    "\n",
    "test_data = []\n",
    "IMG_SIZE_HEIGHT = 1600  # not using \n",
    "IMG_SIZE_WIDTH = 1200   # not using \n",
    "IMG_SIZE = 200         # This is the size we are using \n",
    "\n",
    "def create_test_data():\n",
    "    path = os.path.join(DATADIR2)   # Path to folder \n",
    "\n",
    "    for img in os.listdir(path):\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE) # converts the image to an array \n",
    "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))   # Resize to normalize data size \n",
    "            test_data.append([new_array]) # adds it to are traning data with the label \n",
    "        except Exception as e:\n",
    "            pass\n",
    "            \n",
    "create_test_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a perdiction on the test data and running it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "test_data = np.array(test_data).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "test_data = test_data/255\n",
    "\n",
    "predictions = model.predict_classes(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions)):\n",
    "    print(np.argmax(predictions[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
