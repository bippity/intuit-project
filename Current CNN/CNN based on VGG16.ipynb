{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model on top of VGG16 for image quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import cv2\n",
    "import random \n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the traning data, resizing all images to 200x200 and using color(R,G,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/Users/macbook/OCR/W2\"\n",
    "CATEGORIES = [\"Good\", \"Bad\"]\n",
    "training_data = []\n",
    "IMG_SIZE = 200         # This is the size we are using \n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:                 # loop threw each folder with in W2 folder  \n",
    "        path = os.path.join(DATADIR,category)   # Path to folder \n",
    "        class_num = CATEGORIES.index(category)  # labeling the data based on folder \n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_COLOR) # converts the image to an array \n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))   # Resize to normalize data size \n",
    "                training_data.append([new_array, class_num]) # adds it to are traning data with the label \n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the data before feeding it to the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the features from the labels and converting them to a np array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by normalizing the data by scaling it, min is 0 and max is 255 for pixel data \n",
    "So we will divide it by 255, Keras also has a built in function to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255 #255 pixels max for pixel data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a VGG16 model that is preloaded with weights, \n",
    "\n",
    "We tell it the image size and that the images will be in color(3)\n",
    "\n",
    "include_top = False will not include the classification layer, we will add one ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 200\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "VGG16_MODEL=tf.keras.applications.VGG16(input_shape = IMG_SHAPE,\n",
    "                                               include_top = False,\n",
    "                                               weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the layers VGG16 includes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 212, 212, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 212, 212, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 212, 212, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 106, 106, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 106, 106, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 106, 106, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 53, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 53, 53, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 53, 53, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 53, 53, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 26, 26, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 26, 26, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_MODEL.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are frezzing the VGG16 model so that way the weights in the given model will not update \n",
    "Also including 2 more layers, one being our output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_MODEL.trainable = False\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(2,activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will convert to a sequential model and combine the last two layer we made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  VGG16_MODEL,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile our model using an 'adam' optimizer and 'sparese categorical crossentropy' for the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to fit the model with 5 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3361 samples, validate on 374 samples\n",
      "Epoch 1/5\n",
      "3361/3361 [==============================] - 1358s 404ms/sample - loss: 0.2879 - accuracy: 0.8979 - val_loss: 0.1866 - val_accuracy: 0.9439\n",
      "Epoch 2/5\n",
      "3361/3361 [==============================] - 1354s 403ms/sample - loss: 0.1756 - accuracy: 0.9405 - val_loss: 0.1569 - val_accuracy: 0.9465\n",
      "Epoch 3/5\n",
      "3361/3361 [==============================] - 1344s 400ms/sample - loss: 0.1536 - accuracy: 0.9458 - val_loss: 0.1405 - val_accuracy: 0.9465\n",
      "Epoch 4/5\n",
      "3361/3361 [==============================] - 1177s 350ms/sample - loss: 0.1399 - accuracy: 0.9500 - val_loss: 0.1308 - val_accuracy: 0.9465\n",
      "Epoch 5/5\n",
      "3361/3361 [==============================] - 1112s 331ms/sample - loss: 0.1309 - accuracy: 0.9527 - val_loss: 0.1221 - val_accuracy: 0.9519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6991714d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size = 12, epochs = 5, validation_split = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: VGG16_v1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"VGG16_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR2 = \"/Users/macbook/OCR/testDataGood\"\n",
    "\n",
    "test_data = []\n",
    "IMG_SIZE = 200         # This is the size we are using \n",
    "\n",
    "def create_test_data():\n",
    "    path = os.path.join(DATADIR2)   # Path to folder \n",
    "\n",
    "    for img in os.listdir(path):\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_COLOR) # converts the image to an array \n",
    "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))   # Resize to normalize data size \n",
    "            test_data.append([new_array]) # adds it to are traning data with the label \n",
    "        except Exception as e:\n",
    "            pass\n",
    "            \n",
    "create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array(test_data).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "test_data = test_data/255\n",
    "\n",
    "predictions = model.predict_classes(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
